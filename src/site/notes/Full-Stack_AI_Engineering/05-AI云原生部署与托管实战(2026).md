---
{"dg-publish":true,"permalink":"/full-stack-ai-engineering/05-ai-2026/","noteIcon":""}
---

# 🚀 AI 云原生部署与托管实战 (2026)

> **核心定位：** 从"本地脚本"到"全球访问"的进阶路径。本文档系统阐述 Vercel vs Streamlit 成本与性能、300 秒红线、CDN 加速、自定义域名方案、Docker + Nginx 的"堡垒架构"与异步状态流架构。

**关联文档：** [[Full-Stack_AI_Engineering/Full-Stack_AI_Engineering\|Full-Stack_AI_Engineering]] | [[Full-Stack_AI_Engineering/01-Web基础架构与通讯协议\|01-Web基础架构与通讯协议]] | [[Full-Stack_AI_Engineering/03-服务端逻辑与Agent计算环境\|03-服务端逻辑与Agent计算环境]]

---

## 目录

1. 托管平台选型：Vercel vs Streamlit
2. 部署经济学：成本与限制分析
3. 300 秒红线与异步状态流架构
4. 网络优化：CDN 加速与自定义域名
5. Docker 容器化部署]
6. Nginx 反向代理与网关配置
7. 内网互通与流量成本优化
8. 知识要点总结

---

## 一、托管平台选型：Vercel vs Streamlit

### 1.1 Streamlit：AI 开发者的"零门槛"直通车

在 2026 年的 AI 开发生态中，Streamlit 的地位不可撼动。它最核心的魅力在于：**抹平了前端开发的鸿沟。**

#### "代码即 UI" 的哲学

传统的应用开发需要前端（HTML/CSS/JS）和后端（Python/Node）各一套人马。而 Streamlit 让只会 Python 的 AI 开发者无需"求人"，仅需一行代码（如 `st.chat_input()`）即可生成具有交互能力的聊天界面。

**Streamlit 示例：**
```python
import streamlit as st

st.title("AI 聊天助手")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("输入你的问题"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    # 调用 AI API
    response = call_ai_api(prompt)
    st.session_state.messages.append({"role": "assistant", "content": response})
```

---

#### 为什么它是 Agent 开发的首选？

- **快速验证**：你可以几分钟内给你的 Agent 套上一个类似 ChatGPT 的壳子。
- **状态管理**：它自动处理了按钮点击、输入框变化等状态，让你 100% 专注于 Prompt 和技能逻辑。
- **组件生态**：到 2026 年，Streamlit 已经能支持各种复杂的多模态展示，包括实时视频分析和动态图表。

---

### 1.2 Vercel：静态分发 + 云函数 (Serverless)

虽然两者都能让你在网页上看到应用，但它们的底层运作方式截然不同。

#### Vercel 的本质

- **本质**：它是"**按需分配**"的典范。
- **运行逻辑**：Vercel 把你的前端界面（HTML/JS）推送到全球 CDN。只有当你发起 API 请求（如点击"发送"消息）时，它才会临时拉起一个"云函数"来处理逻辑，处理完立刻关掉。
- **交互特点**：所有的 UI 渲染和简单动画都在用户自己的浏览器里跑，不占用服务器资源。

**Vercel 架构：**
```
用户请求
  ↓
CDN 边缘节点（静态资源）
  ↓
API 路由（Serverless 函数）
  ↓
处理完成后立即关闭
```

---

### 1.3 静态托管 vs 动态托管对比

| **维度** | **Vercel (Next.js/React)** | **Streamlit (Python)** |
|---------|---------------------------|----------------------|
| **UI 自由度** | **100% 自由**。可以完美复刻 Gemini、ChatGPT 的精致布局。 | **30% 自由**。只能在官方提供的侧边栏和组件框架内填空。 |
| **开发门槛** | **较高**。需要掌握 React、HTML/CSS 等前端知识。 | **极低**。懂 Python 就能做出一套带后端的应用。 |
| **并发承载** | **极强**。依托边缘网络，可支持上万人同时在线。 | **一般**。几十个用户同时访问就可能出现明显延迟。 |
| **运行模式** | Serverless（按需启动） | 容器（持续运行） |

---

> [!IMPORTANT]
> 
> **底层洞察：** Vercel 是把后端"切碎"来换取无限的伸缩性和颜值；Streamlit 是把后端"打包"来换取极致的开发速度。

---

## 二、部署经济学：成本与限制分析

### 2.1 Streamlit 部署选项

当你开发完应用点击 **"Deploy now"** 时，系统给出的三个选项代表了完全不同的资源分配策略。

#### Community Cloud (社区云) —— 首选免费方案

- **本质**：由 Streamlit 官方免费提供的托管空间。
- **成本**：**$0**。只要你的代码托管在 GitHub 公开仓库，就可以无限量部署公开应用。
- **限制**：每个应用拥有约 **2.7GB** 的内存配额。如果你的 Agent 只是调用 API，这个内存绰绰有余。

#### Snowflake (企业级) —— 付费商业方案

- **本质**：面向有数据安全性要求、需要处理海量私有数据的企业。
- **成本**：按计算资源消耗计费。
- **价值**：提供无限的私有应用配额及企业级安全防护。

#### Other Platforms (第三方平台) —— 自定义方案

- **本质**：你自己购买服务器（如阿里云 ECS、AWS 等）进行手动部署。
- **成本**：取决于你的服务商。
- **价值**：你可以完全掌控硬件环境，不受官方 300 秒运行或内存限制的影响。

---

### 2.2 2026 部署经济学：到底要不要付费？

对于大多数个人开发者和 AI 创业初期项目，结论是：**不需要付费。**

> **黄金法则：**
> 
> 只要你的应用满足以下两个条件，你就可以一直使用 **Community Cloud** 免费运行：
> 
> 1. 代码在 GitHub 上是公开的（或仅有 1 个私有应用）。
> 2. 你的应用逻辑主要运行在"云端"（即调用 Gemini、GPT 的 API），而非本地加载巨大的模型文件。

---

### 2.3 内存与时间的"红线"

为什么 Vercel 不限内存却"掐时间"，而 Streamlit 给足时间却"爱崩溃"？

#### 内存限制 (Memory Limit)

- **Vercel**：由于采用 Serverless 架构，它几乎**没有传统意义上的"系统内存限制"**。因为它的后端是碎片化的，单次请求只占用极小资源。
- **Streamlit**：拥有严格的 **2.7GB - 3GB RAM 限制**。因为你的应用是一个持续运行的整体。如果你加载了大型本地模型或处理了海量数据，容器会因为内存溢出（OOM）直接崩溃重启。

---

#### 运行时间限制 (Execution Limit)

- **Vercel (300 秒红线)**：在 2026 年，Vercel Fluid Compute 为免费用户提供单次 **300 秒（5 分钟）** 的运行上限。
  
  > **注意**：这 300 秒是"墙钟时间"，包含等待大模型（如 Gemini）返回结果的时间。一旦超时，函数会被强制关闭。

- **Streamlit**：**没有硬性的运行时间限制**。只要内存没爆，你的 Python 脚本可以跑很久。这对处理复杂的长耗时任务（如分析长视频）更有利。

---

### 2.4 2026 托管平台选型矩阵

| **需求场景** | **推荐方案** | **理由** |
|------------|------------|---------|
| **极致颜值、Gemini 式 UI** | **Vercel + Next.js** | 100% 布局自由度，支持流式输出，极致的全球加载速度。 |
| **快速原型、内部工具、Agent 测速** | **Streamlit** | 全 Python 开发，几分钟从逻辑变网页。 |
| **AI 模型展示、多模态（生图/生音）** | **Gradio** | 针对多模态组件深度优化（Hugging Face 官方支持）。 |
| **24/7 持续运行、超长视频处理、长连接** | **阿里云 ECS / Railway** | 真正的虚拟机，没有 300 秒超时限制，100% 环境控制。 |
| **面向国内用户、完全不翻墙** | **自定义域名 + 托管平台** | 解决默认子域名被封的唯一低成本手段。 |

---

## 三、300 秒红线与异步状态流架构

### 3.1 深度拆解：300 秒"墙钟时间"

在 Vercel Fluid Compute 环境下，300 秒（5 分钟）的限制并不是指你的代码执行了 300 秒，而是指**从请求开始到连接结束的总时长**。

- **等待也算时间**：即便你的脚本只跑了 1 秒，但你在 `await` Gemini 响应一个超长视频处理结果时花了 299 秒，你的函数依然会因为撞上 300 秒的墙而被杀掉（Error 504）。
- **Fluid Compute 的计费优化**：虽然它不帮你延长寿命，但它优化了钱包。在"干等"大模型响应的这段时间，它只按"闲置"计费，只有真正的计算时间才按高倍率计费。

**超时示例：**
```javascript
// ❌ 会超时：等待 AI 响应超过 300 秒
export async function POST(request) {
  const response = await fetch('https://api.gemini.com/v1/process-video', {
    method: 'POST',
    body: videoData // 10 分钟的视频
  });
  
  const result = await response.json(); // 等待 10 分钟 → 超时！
  return Response.json(result);
}
```

---

### 3.2 异步（WaitUntil）并不是万能药

很多开发者认为使用 `waitUntil` 或 `after` 开启异步任务就能高枕无忧。这是一个致命的误区。

- **生命周期锁定**：异步任务依然运行在同一个函数实例中。如果你的前端已经收到了"处理中"的响应，但后台的异步任务总时长超过了 `maxDuration`，Vercel 依然会强制掐断这个后台进程。
- **结论**：异步只能让你更早地给用户反馈，但不能让你在云函数里跑"长生不老"的任务。

**异步任务示例：**
```javascript
// ⚠️ 仍然会超时：异步任务超过 300 秒
export async function POST(request) {
  // 立即返回响应
  const taskId = generateTaskId();
  
  // 异步处理（但仍在同一个函数实例中）
  waitUntil(async () => {
    const result = await processLongTask(); // 10 分钟 → 超时！
    await saveToDatabase(taskId, result);
  });
  
  return Response.json({ taskId, status: 'processing' });
}
```

---

### 3.3 安全红线：本地 React 分发的代价

你可能会想：既然云端有 300 秒限制，我直接把 React 应用发给别人，在他们电脑上本地跑不就没限制了？**这会让你面临严重的安全风险。**

#### API Key 的"裸奔"风险

- **前端即公开**：由于 React 在用户浏览器端运行，所有的代码对用户都是透明的。如果你在前端直接调用 Gemini API，任何人只需按下 `F12` 就能在网络请求中直接复制你的 API Key。
- **后果**：别人可以用你的密钥去刷爆你的额度，甚至由于违规调用导致你的开发者账号被封禁。

**关联文档：** [[Full-Stack_AI_Engineering/03-服务端逻辑与Agent计算环境\|03-服务端逻辑与Agent计算环境]] - 安全管理与 API Key 保护

---

#### API 厂商的"第二道墙"

即便你在本地绕过了 Vercel 的 300 秒，你依然绕不过 **Google Gemini API 本身的超时限制**（2026 年约为 5 分钟）。如果大模型处理一个超重负载任务超过这个时间没响应，连接依然会中断。

---

### 3.4 2026 年长耗时任务的"标准答案"

处理超长 AI 任务（如处理 10 分钟以上的视频或生成长篇研报），专业开发者通常采用 **"异步状态流"架构**：

#### 架构流程

1. **任务提交**：用户上传视频，Vercel 接收请求后立即存入数据库，并返回一个 `task_id`（耗时 < 1s）。
2. **外部队列**：使用 **Upstash QStash** 或 **Inngest** 等工具。它们会接手这个任务，在不受 Vercel 限制的后台环境中慢慢调大模型。
3. **轮询/推送**：
   - **前端轮询**：每隔 5 秒问一次："任务 ID 为 XXX 的好了吗？"
   - **Webhook 推送**：任务完成后，后端主动通知前端。

**异步状态流架构示例：**
```javascript
// 1. 提交任务（Vercel API）
export async function POST(request) {
  const { videoUrl } = await request.json();
  const taskId = generateTaskId();
  
  // 立即保存任务到数据库
  await db.tasks.create({
    id: taskId,
    status: 'pending',
    videoUrl
  });
  
  // 发送到外部队列（不受 300 秒限制）
  await qstash.publish({
    url: 'https://your-worker.upstash.io/process',
    body: { taskId, videoUrl }
  });
  
  // 立即返回（< 1 秒）
  return Response.json({ taskId, status: 'processing' });
}

// 2. 外部队列处理（Upstash Worker，不受 300 秒限制）
export async function processTask({ taskId, videoUrl }) {
  // 这里可以运行 10 分钟、1 小时，不受限制
  const result = await processLongVideo(videoUrl);
  
  // 更新数据库
  await db.tasks.update(taskId, {
    status: 'completed',
    result
  });
  
  // 可选：Webhook 通知前端
  await notifyFrontend(taskId, result);
}

// 3. 前端轮询状态
function PollTaskStatus({ taskId }) {
  const [status, setStatus] = useState('processing');
  
  useEffect(() => {
    const interval = setInterval(async () => {
      const response = await fetch(`/api/tasks/${taskId}`);
      const data = await response.json();
      setStatus(data.status);
      
      if (data.status === 'completed') {
        clearInterval(interval);
        setResult(data.result);
      }
    }, 5000); // 每 5 秒轮询一次
    
    return () => clearInterval(interval);
  }, [taskId]);
  
  return <div>状态：{status}</div>;
}
```

---

> [!CAUTION]
> 
> **安全警示：**
> 
> 绝对不要在纯前端（React/Vue）代码中硬编码任何 API Key。即使是分发给"信任的人"，也要通过一个简单的本地 Node.js 后端进行中转，以保护你的数字资产。

---

## 四、网络优化：CDN 加速与自定义域名

### 4.1 什么是 CDN？互联网的"前置仓"

**CDN (Content Delivery Network)**，即内容分发网络。如果说你的服务器（Origin Server）是位于纽约的披萨总部，那么 CDN 就是它在全球各大城市开设的**外卖配送站（边缘节点）**。

#### 核心定义

CDN 是分布在地理上的服务器组。它的任务是：**把离你最近的内容，以最快的速度发给你。**

#### 三个关键角色

- **源站 (Origin Server)**：你存放原始代码和数据的地方（如 Vercel 的主数据库）。
- **边缘节点 (Edge Nodes/PoPs)**：分布在全球各地的缓存服务器。
- **缓存 (Caching)**：将源站的"冷冻半成品"（静态文件、图片、脚本）复印一份存放在配送站。

---

### 4.2 全球 CDN 的工作机制：Anycast 与智能路由

在 2026 年，全球 CDN 不仅仅是简单的文件存储，它拥有极其复杂的调度逻辑：

1. **Anycast (任播) 技术**：当你在浏览器输入你的域名时，CDN 会通过 Anycast 技术，让全球数百个节点共享同一个 IP。网络会自动将你的请求引导至物理距离最近、且最不拥堵的那个节点。
2. **智能路由 (Smart Routing)**：如果北京的节点坏了，系统会在几毫秒内自动切换到上海或东京，用户完全感知不到故障。
3. **边缘计算 (Edge Computing)**：到 2026 年，CDN 节点不再只是"存东西"。像 Vercel 的 **Edge Functions** 允许你在 CDN 节点上运行轻量级代码（如身份验证、重定向），无需回传到远在美国的总部服务器。

---

### 4.3 2026 年 CDN 的新特性：AI 预测加载

2026 年的 CDN 变得更加智能，加入了 **AI 预测性加载 (Predictive Loading)**：

- **行为分析**：CDN 会分析该地区用户的习惯。如果 80% 的用户看完首页都会点"产品介绍"，边缘节点会预先从源站调取该页面并缓存。
- **Edge AI 推理**：部分轻量级的 AI 模型（SLM，小语言模型）直接运行在 CDN 边缘。这意味着你的"英语教育 Wearable"应用可以实现在边缘节点进行语音识别，延迟低至 10ms 级。

---

### 4.4 为什么 AI 开发者离不开 CDN？

1. **降低 API 成本**：如果 1000 个用户都问同样的问题，CDN 可以缓存大模型的回答（针对非个性化内容），从而节省昂贵的 Token 费用。
2. **防御 DDoS 攻击**：CDN 就像一面巨大的海绵，能吸收掉海量的恶意洪水流量，保护你脆弱的源站后端。
3. **Core Web Vitals (网站核心指标)**：SEO 权重与页面加载速度挂钩。Vercel 自带的全球 CDN 能让你即便在全球分发应用，也能获得近乎满分的加载评分。

---

### 4.5 Vercel vs Streamlit 的 CDN 表现对比

- **Vercel**：原生集成了全球顶级 CDN。你每点一次 `git push`，Vercel 会自动将内容分发至全球 **100+ 个边缘 PoP 节点**。它是目前市面上 CDN 整合度最高的平台之一。
- **Streamlit**：依赖于单点容器。虽然它也有简单的网络加速，但由于后端逻辑必须回到容器内运行，它的"边缘化"程度远不如 Vercel。

---

### 4.6 网络黑盒：为何你的链接需要 VPN？

当你将应用上传到 Vercel 或 Streamlit 后，平台会分配一个形如 `your-app.vercel.app` 或 `your-app.streamlit.app` 的子域名。但在国内，你很快会发现：**不挂 VPN 根本打不开。**

#### 默认子域名的"原罪"

这并非平台在收费上为难你，而是源于复杂的网络环境：

- **DNS 污染与 SNI 封锁**：由于 Vercel 和 Streamlit 的默认后缀（`.vercel.app` / `.streamlit.app`）承载了全球海量的各类内容，它们在 2026 年依然处于国内防火墙的重点拦截名单中。
- **IP 封锁**：这些托管平台通常使用共享 IP 池。如果同一个 IP 下的其他应用违规，可能会导致该 IP 整个段在国内无法访问。

---

### 4.7 零成本/低成本"免翻墙"方案

要解决访问问题，核心思路是**"换马甲"**——用你自己的域名替代平台的默认域名。

#### 方案 A：绑定自定义域名（最有效、最推荐）

这是 2026 年最主流的避坑策略：

1. **购买廉价域名**：在阿里云或腾讯云买一个 `.top`、`.site` 或 `.asia` 域名（首年通常只需几块钱）。
2. **配置 DNS 解析**：在域名后台添加一条 `CNAME` 记录，指向 Vercel 或 Streamlit 提供的地址。
3. **原理**：自定义域名不在屏蔽名单中，且 Vercel 的**边缘网络（Edge Network）**会自动感应到你的请求，并从离你最近的海外节点（如香港、新加坡、日本）把内容发给你。

**DNS 配置示例：**
```
类型：CNAME
主机记录：@ 或 www
记录值：cname.vercel-dns.com
```

---

#### 方案 B：使用 Cloudflare CDN 加速

如果你的域名在直连时依然不够稳定，可以利用 Cloudflare 作为中转站：

- 开启 Cloudflare 的"小云朵"代理。
- 流量路径变为：**用户 -> Cloudflare 节点 -> Vercel 服务器**。这能有效隐藏原始服务器特征，提升访问稳定性。

---

### 4.8 Vercel vs Streamlit：访问体验的细微差别

虽然都需要绑定域名，但两者的底层网络基建不同：

- **Vercel 的优势**：拥有极其强大的全域 CDN 节点。一旦绑定了自定义域名，其"边缘网络"能让国内访问速度非常接近原生网站。
- **Streamlit 的劣势**：由于其后端必须运行在一个特定的物理容器里（而非分布式的边缘函数），它的服务器节点相对固定。即便绑定了域名，国内访问的延迟（Ping 值）通常也高于 Vercel。

---

> [!TIP]
> 
> **实战建议：** 如果你是为了给国内用户演示，请不要吝啬那几块钱的域名费用。**"买个域名并绑定"**是目前解决所有访问玄学问题的唯一、直接且最省成本的方案。

---

## 五、Docker 容器化部署

### 5.1 Docker：软件界的"标准集装箱"

如果你把代码比作"货物"，那么 Docker 就是那个**集装箱**。

#### 核心定义

Docker 是一种**容器化技术**。它不只是打包你的 Python 代码，而是把代码、运行环境（Python 3.10）、依赖库（LangChain, FastAPI）、甚至是操作系统最核心的文件全部封装在一起。

**关联文档：** [[Full-Stack_AI_Engineering/03-服务端逻辑与Agent计算环境\|03-服务端逻辑与Agent计算环境]] - Docker 容器化与沙箱隔离

---

#### 为什么开发者离不开 Docker？

- **解决环境差异**：你的电脑是 Windows/Mac，服务器是 Linux。没有 Docker 时，你经常会遇到"在我电脑上明明能跑"的报错。有了 Docker，集装箱在哪打开都一样。
- **轻量化**：相比传统的虚拟机（VM），Docker 容器非常小巧。它共用宿主机的内核，启动仅需几秒钟。
- **可移植性**：你可以直接把在本地打包好的镜像（Image）推送到阿里云镜像仓库，然后在 ECS 上一键拉取运行。

---

### 5.2 Dockerfile 编写示例

**Python FastAPI 应用 Dockerfile：**
```dockerfile
# 使用官方 Python 运行时作为基础镜像
FROM python:3.10-slim

# 设置工作目录
WORKDIR /app

# 复制依赖文件
COPY requirements.txt .

# 安装依赖
RUN pip install --no-cache-dir -r requirements.txt

# 复制应用代码
COPY . .

# 以非 root 用户运行（增强安全性）
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# 暴露端口
EXPOSE 8000

# 运行应用
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Node.js 应用 Dockerfile：**
```dockerfile
FROM node:18-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY . .

EXPOSE 3000

CMD ["node", "server.js"]
```

---

### 5.3 Docker Compose 多服务编排

**docker-compose.yml：**
```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/mydb
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - db
    volumes:
      - ./data:/app/data:ro
    restart: unless-stopped

  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=mydb
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  postgres_data:
```

---

### 5.4 Docker 在 ECS 上的部署

**关联文档：** [[Full-Stack_AI_Engineering/03-服务端逻辑与Agent计算环境\|03-服务端逻辑与Agent计算环境]] - Docker 容器化与沙箱隔离

---

## 六、Nginx 反向代理与网关配置

### 6.1 Nginx：你的服务器"超级管家"

**Nginx** 是一款高性能的 HTTP 和反向代理服务器。如果把你的 Docker 容器比作山里的秘密实验室，Nginx 就是山门口唯一的守卫。

**关联文档：** [[Full-Stack_AI_Engineering/01-Web基础架构与通讯协议\|01-Web基础架构与通讯协议]] - CORS 跨域资源共享机制

---

### 6.2 核心职能：什么是反向代理 (Reverse Proxy)？

这是 Nginx 最重要的工作。

- **原理**：外部用户访问你的域名（如 `api.yoursite.com`）时，其实是访问了 Nginx。Nginx 收到请求后，根据你写好的规则，把请求转发给躲在后台的 Docker 容器（比如 `127.0.0.1:8000`）。
- **价值**：**隐藏真实地址**。外界永远不知道你真正的后端跑在哪个端口，甚至不知道你用的是什么编程语言，极大地提升了安全性。

**反向代理示意图：**
```
用户请求 → api.yoursite.com:443
           ↓
        Nginx (443端口)
           ↓
    Docker 容器 (8000端口)
```

---

### 6.3 Nginx 的"语言"：是配置而非编程

正如你所察觉的，Nginx 用的不是 Python 或 Java 那种编程语言，而是一种 **DSL (领域特定语言)**。

- **特点**：它是一种基于"指令"的配置语法。你不需要写复杂的循环或算法，你只需要"填表"一样告诉它规则。
- **门槛**：非常低，但要求极高的准确性（漏掉一个分号 `;` 就会导致服务无法启动）。

---

### 6.4 SSL 卸载：给你的 API 戴上"绿锁"

在 2026 年，所有 API 必须使用 HTTPS（加密传输）。

- **脏活累活**：加解密是非常耗费 CPU 资源的。
- **解决方案**：Nginx 负责处理所有的加密工作（SSL 证书挂在 Nginx 上）。当请求通过 Nginx 后，它会以极速的 HTTP 方式传给你的 Python 后端。这让你的 Agent 逻辑可以专注于推理，而不必分心去处理加密逻辑。

---

### 6.5 Nginx 配置示例

**基础反向代理配置：**
```nginx
server {
    listen 80;
    server_name api.yoursite.com;

    # 反向代理到后端
    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

**HTTPS + CORS 配置：**
```nginx
server {
    listen 443 ssl http2;
    server_name api.yoursite.com;

    # SSL 证书配置
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;

    # CORS 配置
    add_header 'Access-Control-Allow-Origin' 'https://frontend.com' always;
    add_header 'Access-Control-Allow-Methods' 'GET, POST, PUT, DELETE, OPTIONS' always;
    add_header 'Access-Control-Allow-Headers' 'Content-Type, Authorization' always;

    # 处理预检请求
    if ($request_method = 'OPTIONS') {
        return 204;
    }

    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}

# HTTP 重定向到 HTTPS
server {
    listen 80;
    server_name api.yoursite.com;
    return 301 https://$server_name$request_uri;
}
```

**多应用路由配置：**
```nginx
server {
    listen 443 ssl http2;
    server_name api.yoursite.com;

    # 聊天 Agent
    location /chat {
        proxy_pass http://127.0.0.1:8001;
    }

    # 画图 Agent
    location /draw {
        proxy_pass http://127.0.0.1:8002;
    }

    # 默认路由
    location / {
        proxy_pass http://127.0.0.1:8000;
    }
}
```

---

### 6.6 为什么企业喜欢在 ECS 前面套 Nginx？

即使阿里云提供了各种"一键部署"前端的工具（如 OSS + CDN），企业依然习惯在 ECS 前放一个 Nginx，原因有三：

1. **内网互通的极速**：
   
   如果你的 Nginx 和后端逻辑都在同一个 ECS 或同一个 VPC（虚拟私有云）内，它们之间的通信走的是"内部专用通道"，速度远快于通过公网。

2. **多应用路由**：
   
   你可以在一个 ECS 上跑 5 个不同的 Docker 容器（比如一个聊天 Agent，一个画图 Agent）。Nginx 可以根据 URL（如 `/chat` 或 `/draw`）精准地把用户送到对应的容器里。

3. **流量过滤与防刷**：
   
   Nginx 可以配置"限流"策略。如果某个 IP 在一秒内请求了 100 次，Nginx 会直接把它拦在门口，防止你的大模型额度被瞬间刷爆。

**限流配置示例：**
```nginx
# 定义限流区域
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;

server {
    location /api {
        # 应用限流
        limit_req zone=api_limit burst=20 nodelay;
        
        proxy_pass http://127.0.0.1:8000;
    }
}
```

---

## 七、内网互通与流量成本优化

### 7.1 为什么企业倾向于"全家桶"部署？

虽然 Vercel 的前端体验极佳，但企业往往会将前后端都迁入同一个云生态，原因在于：

- **内网互通 (Intranet)**：同一厂商的服务器之间走的是内部带宽。数据传输不经过公网，速度接近物理极限，且**安全性极高**。
- **流量成本**：云厂商通常对"入方向"流量免费，但对"出方向"公网流量收费。前后端同源可以大幅减少昂贵的公网带宽支出。
- **绕过 CORS**：利用 Nginx，可以将前端和后端挂在同一个域名下（如 `/` 是前端，`/api` 是后端）。这种**"同源部署"**从根源上消灭了跨域问题，不再需要复杂的配置。

**关联文档：** [[Full-Stack_AI_Engineering/01-Web基础架构与通讯协议\|01-Web基础架构与通讯协议]] - CORS 跨域资源共享机制

---

### 7.2 内网互通架构示例

**阿里云 VPC 内网架构：**
```
┌─────────────────────────────────┐
│        阿里云 VPC 内网             │
│                                  │
│  ┌──────────┐    ┌──────────┐   │
│  │  Nginx   │───▶│ 后端 API │   │
│  │ (ECS-1)  │    │ (ECS-2)  │   │
│  └──────────┘    └──────────┘   │
│       │                │         │
│       └────────┬───────┘         │
│                │                 │
│         ┌──────▼──────┐         │
│         │  数据库 RDS  │         │
│         │  (内网地址)  │         │
│         └─────────────┘         │
│                                  │
│  所有通信走内网，不经过公网        │
└─────────────────────────────────┘
```

**内网配置示例：**
```nginx
# Nginx 配置：使用内网地址访问后端
server {
    location /api {
        # 使用内网 IP（不经过公网，免费且快速）
        proxy_pass http://172.16.0.10:8000;
    }
}
```

---

### 7.3 流量成本对比

| **场景** | **流量路径** | **成本** |
|---------|------------|---------|
| **前后端分离（不同云）** | 前端 → 公网 → 后端 | 出方向流量收费（$0.XX/GB） |
| **前后端同源（同云内网）** | 前端 → 内网 → 后端 | 免费（内网流量） |
| **前后端同机（Nginx）** | 前端 → 本地 → 后端 | 免费（本地通信） |

---

## 八、知识要点总结

### 托管平台对比

| **平台** | **运行模式** | **UI 自由度** | **开发门槛** | **适用场景** |
|---------|------------|-------------|------------|------------|
| **Vercel** | Serverless | 100% | 较高 | 极致颜值、全球分发 |
| **Streamlit** | 容器 | 30% | 极低 | 快速原型、AI 工具 |
| **ECS + Docker** | 虚拟机 | 100% | 中等 | 24/7 运行、长任务 |

---

### 部署限制与解决方案

| **限制** | **Vercel** | **Streamlit** | **ECS** |
|---------|-----------|--------------|---------|
| **运行时间** | 300 秒 | 无限制 | 无限制 |
| **内存限制** | 无（Serverless） | 2.7GB | 自定义 |
| **解决方案** | 异步状态流 | - | - |

---

### CDN 与网络优化

| **技术** | **作用** | **优势** |
|---------|---------|---------|
| **CDN** | 内容分发 | 全球加速、降低延迟 |
| **自定义域名** | 绕过封锁 | 解决国内访问问题 |
| **内网互通** | 流量优化 | 免费、高速、安全 |

---

### Docker 与 Nginx

| **工具** | **职能** | **核心价值** |
|---------|---------|------------|
| **Docker** | 容器化 | 环境一致性、可移植性 |
| **Nginx** | 反向代理 | 安全网关、负载均衡、SSL 卸载 |

---

### 架构建议

**小型项目：**
- 前端：Vercel
- 后端：Vercel Serverless Functions
- 数据库：Vercel Postgres / Upstash Redis

**中型项目：**
- 前端：Vercel
- 后端：阿里云 ECS + Docker + Nginx
- 数据库：阿里云 RDS

**大型项目：**
- 前端：Vercel / 阿里云 OSS + CDN
- 后端：阿里云 ECS 集群 + Docker + Nginx + 负载均衡
- 数据库：阿里云 RDS 主从 + Redis 集群

---

**下一步学习：**
- 回顾架构基础：[[Full-Stack_AI_Engineering/01-Web基础架构与通讯协议\|01-Web基础架构与通讯协议]]
- 回顾后端原理：[[Full-Stack_AI_Engineering/03-服务端逻辑与Agent计算环境\|03-服务端逻辑与Agent计算环境]]
- 返回索引：[[Full-Stack_AI_Engineering/Full-Stack_AI_Engineering\|Full-Stack_AI_Engineering]]

---
[[Full-Stack_AI_Engineering/Full-Stack_AI_Engineering\|Full-Stack_AI_Engineering]]
